If @sigma@ had in fact been a step function, then the sigmoid neuron would _be_ a perceptron, since the output would be @1@ or @0@ depending on whether @w* x+b@ was positive or negative* <span class="marginnote">*Actually, when @w * x +b = 0@ the perceptron outputs @0@, while the step function outputs @1@. So, strictly speaking, we'd need to modify the step function at that one point. But you get the idea.</span>. By using the actual @sigma@ function we get, as already implied above, a smoothed out perceptron. Indeed, it's the smoothness of the @sigma@ function that is the crucial fact, not its detailed form. The smoothness of @sigma@ means that small changes @Delta w_j@ in the weights and @Delta b@ in the bias will produce a small change @Delta output@ in the output from the neuron. In fact, calculus tells us that @Delta output@ is well approximated by

<a class="displaced_anchor" name="eqtn5"></a>

%Delta output ≈ sum_j(deloutput)/(delw_j) Delta w_j + (del output)/(del b) Delta b,        (5)%

where the sum is over all the weights, @w_j@, and @(deloutput)/(del w_j)@ and @(deloutput)/(delb)@ denote partial derivatives of the output with respect to @w_j@ and @b@, respectively. Don't panic if you're not comfortable with partial derivatives! While the expression above looks complicated, with all the partial derivatives, it's actually saying something very simple (and which is very good news): @Delta output@ is a _linear function_ of the changes @Delta w_j@ and @Delta b@ in the weights and bias. This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change in the output. So while Neuronas Sigmoid have much of the same qualitative behaviour as perceptrons, they make it much easier to figure out how changing the weights and biases will change the output.

If it's the shape of @sigma@ which really matters, and not its exact form, then why use the particular form used for @sigma@ in Equation <span id="margin_850263336921_reveal" class="equation_link">(3)</span><span id="margin_850263336921" class="marginequation" style="display: none;"> [ @sigma(z) == 1/{1+e^{-z}}@ ](#eqtn3) </span> ? In fact, later in the book we will occasionally consider neurons where the output is @f(w * x + b)@ for some other _activation function_ @f(*)@. The main thing that changes when we use a different activation function is that the particular values for the partial derivatives in Equation
<span id="margin_444952422305_reveal" class="equation_link">(5)</span><span id="margin_444952422305" class="marginequation" style="display: none;">[ @Delta output ≈ sum_j(deloutput)/(delw_j) Delta w_j + (del output)/(del b) Delta b@ ](#eqtn5)</span> change. It turns out that when we compute those partial derivatives later, using @sigma@ will simplify the algebra, simply because exponentials have lovely properties when differentiated. In any case, @sigma@ is commonly-used in work on neural nets, and is the activation function we'll use most often in this book.

How should we interpret the output from a sigmoid neuron? Obviously, one big difference between perceptrons and Neuronas Sigmoid is that Neuronas Sigmoid don't just output @0@ or @1@. They can have as output any real number between @0@ and @1@, so values such as @0.173...@ and @0.689...@ are legitimate outputs. This can be useful, for example, if we want to use the output value to represent the average intensity of the pixels in an image input to a neural network. But sometimes it can be a nuisance. Suppose we want the output from the network to indicate either "the input image is a 9" or "the input image is not a 9". Obviously, it'd be easiest to do this if the output was a @0@ or a @1@, as in a perceptron. But in practice we can set up a convention to deal with this, for example, by deciding to interpret any output of at least @0.5@ as indicating a "9", and any output less than @0.5@ as indicating "not a 9". I'll always explicitly state when we're using such a convention, so it shouldn't cause any confusion.
