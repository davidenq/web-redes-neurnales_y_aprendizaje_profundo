<p>hola mundo</p>
El sistema humano visual es una de las grandes maravillas en el mundo. Considera la siguiente secuencia de dígitos escritos a mano: <a name="complete_zero">ss</a>

<center>![](http://localhost:3000/public/assets/images/digits.png)</center>

Con muy poco esfuerzo, la mayoria de la gente puede reconocer esos números e indicar que son: 504192, pero esto es fácilmente engañoso.
En cada hemisferio de nuestro cerebro, los seres humanos tenemos una cortesa visual primaría; es conocida como V1 y contiene aproximadamente unas 140 milliones de neuronas con aproximadamente unas 10 billones de conexiones entre ellas. Sin embargo, la visión humana no implica una sola V1, sino una serie de cortezas visuales - V2, V3, V4 y V5 - haciendo progresivamente procesamiento de imágenes más complejas. Llevamos en nuestra cabeza un supercomputador ajustado por la evolución de más de cien millones de años y magníficamente adaptandose para comprender el mundo visual.

El reconocimiento de dígitos escritos a mano no es una tarea sencilla, por el contrario, los seres humanos somos asombrosos y sorprendentemente buenos para hacer que las cosas tengan sentido de lo que nuestros ojos nos muestran, no obstante, casi todo el trabajo se lo realiza de una forma inconsciente. Y usualmente no somos capaces de apreciar como, nuestro sistema visual, resuelve un problema bastante difícil de una forma relativamente sencilla.

La dificultad visual que conlleva el reconocimiento de patrones se hace bastante evidente si se intenta escribir un programa de computador que realize el trabajo de reconocer los dígitos escritos a mano, como los mostrados anterioremente. Lo que parece fácil cuando lo hacemos notroso mismos de repente se vuelve extremadamente difícil. Una simple intuición acerca de como reconocer formas - "un 9 tiene un lazo en la parte superior y una barra de forma verticial en la parte inferior derecha" - sin embargo, esta descripción, resulta no ser tan simple como parece para poder ser expresada a través de un algoritmo. Cuando se intenta realizar de forma precisa esas reglas, se perderá rápidamente en una maraña de excepciones, salvedades y casos especiales. Esto parece no tener esperanza alguna.

Las redes neuronales atácan el problema de una manera diferente. La idea es tomar una gran cantidad de dígitos escritos a mano que servirán como patrones o ejemplos de entrenamiento,

<center>![](public/assets/images/mnist_100_digits.png)</center>

y luego desarrollar un sistema el cuál pueda aprender de estos ejemplos de entrenamiento. En otras palabras, la red neuronal usará estos ejemplos y automáticamente podrá deducir reglas de tal manera que le permitan reconocer dígitos escritos a mano. Además, si se incrementa el número de ejemplos de entrenamiento, la red puede aprender más y así mejorar su exactitud a la hora del reconocimiento. Así que mientras he mostrado anteriormente sólo 100 dígitos de entrenamiento, talvés podríamos hacer un mejor sistema de reconocimiento mediante el uso de miles, o incluso millones o miles de millones de ejemplos de entrenamiento.

En este capítulo escribiremos un programa implementando una red neuronal que aprenda a reconocer dígitos escritos a mano. El programa tiene únicamente 74 líneas de código y no usa ninguna librería de redes neuronales en particular. Sin embargo, este pequeño programa podrá reconocer dígitos escritos a mano con una exactitud de más del 96 por ciento sin intervensión humana. Además en capítulos posteriores desarrollaremos las ideas permitiendo una mejora de hasta el 99 por ciento del reconocimiento de dígitos escritos a mano. De hecho, las mejores redes neuronales comerciales son tan buenas que son utilizadas por bancos para procesar cheques y por las oficinas de correo postal para reconocer direcciones.

Estarémos enfocados en el reconocimiento de escritura a mano puesto que es un excelente problema prototípo para aprender de forma general acerca de las redes neuronales. Como un prototipo que predonima, es importante mencionar sobre un punto importante: Es un desafío- no es una tarea sencilla reconocer dígitos escritos a mano - pero tampoco es demasiado dificil, no requeriere una solución extremadamente complicada con un enorme poder computacional. Además, es un buen inicio, en gran manera, para desarrollar técnicas avanzadas como el aprendizaje profundo, por ejemplo. Y así, através del libro volveremos repetidas veces a este problema - "reconocimiento de escritura". Más adelante en el libro, discutiremos cómo estas ideas pueden aplicarse a otros problemas de visión por computador y también en el habla, el procesamiento del lenguaje natural y otros dominos.

Por supuesto, si el punto principal del capítulo fue sólo escribir un programa de ordenador para realizar el reconocimiento de dígitos escritos a mano, entonces el capítulo sería mucho más corto. Pero esto no es así puesto que en el transcurso de este libro desarrollaremos muchas ideas importantes sobre las redes neuronales, incluyendo dos importantes tipos de neurona artificial (el perceptrón y la neurona sigmoide) y el algoritmo estándar de aprendizaje de redes neuronales conocido como descenso de gradiente estocástico. Además, me centraré en explicar _por qué_ se hacen las cosas de esa manera y en la construcción y percepción de una red neuronal. Todo esto requiere una larga discusión a diferencia de si solo se presentara la mecánica básica de lo que está haciendo, pero vale la pena porque tendrás una compresión mucho más profunda. Al finalizar el capítulo estaremos en una posición tal que nos permitirá entender que es el aprendizaje profundo y por qué es tan importante.

### <a name="perceptrons"></a>[Percentrones](chap1.html#perceptrons)

¿Qué es una red neuronal? Para empezar, explicaré un tipo de neurona artificial llamada _percentron_. El perceptron fue [desarrollado](http://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ) entre los años 1950 y 1960 por el científico [Frank Rosenblatt](http://en.wikipedia.org/wiki/Frank_Rosenblatt), quién fue inspirado por el [trabajo](http://scholar.google.ca/scholar?cluster=4035975255085082870) realizado por [Warren McCulloch](https://es.wikipedia.org/wiki/Warren_McCulloch) y [Walter Pitts](http://en.wikipedia.org/wiki/Walter_Pitts), y que trata acerca de las teorías del cerebro y la neurona como la unidad más básica del cerebro. Hoy en día, es más comun usar otro modelo de neurona artificial - en este libro, y en otros muchos y modernos trabajos sobre redes neuronales, el principal modelo de neurona usada es llamada _neurona sigmoide_. Irémos en breve a tocar el tema de las Neuronas Sigmoide. Pero para entender porque son definidas de esa manera, vale la pena tomarse el tiempo necesario y entender primero el percentron.

Así que ¿Cómo trabaja un percetron? Un percetrón tiene algunas entradas binarias, @x_1, x_2, \ldots@, y produce una única salida binaria:

<center>![](public/assets/images/tikz0.png)</center>

En el ejemplo se muestra un percentron con 3 entradas @x_1, x_2, x_3@. En general, esto podría tener menos o más entradas. Rosenblatt introdujo los llamados _pesos_, @w_1,w_2, w_3,\ldots@, que no son más que valores numéricos asociados a cada una de sus correspondientes entradas, además estos pesos expresan la importancia de cada valor de entrada. A la salida de la neurona se tiene uno de dos posibles valores, @0@ o @1@. Este valor queda determinado por el resultado de la suma ponderada entre el producto de los pesos y los parámetros de entrada. Matemáticamente queda expresado como:

<center>@\sum_j w_j x_j@</center>

Es decir, si este resultado es menor o mayor que un cierto _valor umbral_, entonces se tendrá a la salida o @0@ o @1@. Rosenblatt lo propuso como regla simple para calcular la salida. Para definirlo en términos matemáticos más precisos se tiene que: <a class="displaced_anchor" name="eqtn1"></a>\begin{eqnarray} \mbox{salida} & = & \left\{ \begin{array}{ll} 0 & \mbox{si } \sum_j w_j x_j \leq \mbox{ umbral} \\ 1 & \mbox{si } \sum_j w_j x_j > \mbox{ umbral} \end{array} \right. \tag{1}\end{eqnarray} Esto es todo lo que hay acerca de cómo funciona un perceptron. Como se puede observar en la ecuación anterior, es un modelo matemático bastante sencillo.

Una manera en la que puedes pensar acerca del perceptron es que es un dispositivo que toma desiciones examinando los datos de entrada. Permíteme dar un ejemplo, pero no es un ejemplo muy realista, sin embargo, es fácil de entender, y pronto daré ejemplos mucho más realistas.

Supongamos que se acerca el fin de semana, y has oído que se va a llevar a cabo en tu ciudad un festival de queso, y a tí, te encanta el queso. Entonces, estás tratando de decidir si vas o no al festival. Para tomar una decisión debes examinar tres factores importantes:

1.  ¿El clima será bueno? ¿Hará un bonito día?
2.  ¿Tu enamorada (o) deseará acompañarte?
3.  ¿Hay transporte público cerca del festival? (No tienes un vehículo propio).

Podemos representar esos tres factores que incidirán en tu respuesta por una correspondiente variable binaria @x_1, x_2@, y @x_3@. Por ejemplo, tenemos @x_1 = 1@ si el clima es bueno, y @x_1 = 0@ si no lo es. De forma similar, @x_2 = 1@ si tu enamorada(o) desea ir contigo, y @x_2 = 0@ si no desea. Y nuevamente, de forma similar para el transporte público y su correspondiente variable binaria @x_3@.

Ahora bien, suponiendo que verdaderamente adoras tanto el queso que estarías muy contento de ir al festival incluso si tu enamorada(o) no le interesa ir por cualquiera que fuera el motivo, o aun si es difícil llegar al festival. Pero por otro lado, talvés detestas tanto el mal tiempo que no habría manera de que vayas al festival si es que realmente no hace un bonito día. Puedes usar el modelo perceptron para este tipo de decisiones. Una manera de hacer esto es seleccionar un peso @w_1 = 6@ para el clima y @w_2 = 2@ y @w_3 = 2@ para los otros casos, respectivamente. El valor más grande que corresponde a @w_1@, indica que el clima tiene mayor importancia, inclusive más importante que si tu enamorada(o) decide ir o no contigo, o si es que hay o no tranporte disponible y cercano al festival. Por último, supongamos que elegiste 5 como el valor umbral (límite). Con estos valores seleccionados, el percetrón implementa el modelo deseado para la toma de decisiones, obtener @1@ cada vez que el clima es bueno, y @0@ si no lo es. No importa que se obtenga a la salida con los otros factores, es decir, si tu enamorada(a) quiere ir o no, o si hay transporte disponible y cercano o no lo hay.

Al variar los pesos y el valor de umbral, podemos conseguir diferentes resultados en la toma de decisiones. Por ejemplo, supongamos que en lugar de escoger @5@ como el valor umbral, ahora elíges @3@. Entonces, el perceptron podría decedir que deberías ir al festival siempre que haga un buen día _o_ cuando los otros dos factores se cumplan, es decir, habrá transporte público disponible _y_ tu enamorada(o) está dispuesta(o) a ir contigo. En otras palabras, sería un modelo de toma de decisiones diferente puesto que el valor de umbral cáe en un valor medio pues estás más dispuesto a ir al festival.

Obviamente, ¡el perceptron no es un modelo completo de toma de decisiones humana! Pero en la ilustración del ejemplo, vemos como un perceptron puede ponderar diferentes tipos de certidumbres para tomar decisiones. Y debe parecer meritorio que una compleja red de perceptrones pueda tomar decisiones mucho más sutiles:

<center>![](public/assets/images/tikz1.png)</center>

En esta red, la primera columna de perceptrones - lo que llamarémos la primera _capa_ de perceptrones - esta tomando 3 decisiones muy simples, ponderando los parámetros de entrada, particularmente para este caso son 5\. Y ¿En cuanto a la segunda capa de perceptrones? Bueno, cada uno de estos perceptrones está tomando una decisión a través de la ponderación del resultado de la primera capa que previamente ya tomó decisiones. De este modo un perceptrón en la segunda capa, en comparación con uno de la primera, puede tomar una decisión con un nivel de abstración mucho más complejo. Incluso, decisiones más complejas pueden ser hechas por el perceptrón de la tercera capa. Así, una red de muchas capas de perceptrones puede participar en una muy elaborada toma de decisiones.

A propósito, cuando definí percetrones, dije que un percetrón tiene una única salida. En la red anterior de perceptrones se puede observar como cada perceptrón tiene multiples salidas. De hecho no son varias salidas, sino es una única salida, pero esa única salida va como entrada a cada perceptrón de la siguiente capa. Multiples flechas de salida son simplemente un modo útil de representar que esa salida formará parte de una entrada en cada percetrón. Es menos difícil que trazar una simple línea de salida el cual luego se divide en más lineas.

Simplifiquemos la manera en que describimos a los percetrones. La condición @\sum_j w_j x_j > \mbox{umbral}@ es un poco "incómoda", y podemos hacer dos cambios notables para simplificar esto. El primera cambio es escribir @\sum_j w_j x_j@ como un producto punto, es decir, @w \cdot x \equiv \sum_j w_j x_j@, donde @w@ y @x@ son vectores cuyas componentes son los pesos y las entradas, respectivamente. El segundo cambio es mover el valor umbral al otro lado de la desigualdad, y sustituirlo por lo que se conoce como _bías_ de un percetrón, @b \equiv -\mbox{umbral}@. Usando _bías_ en lugar del valor umbral, la formula del perceptrón ahora se puede escribir como sígue: <a class="displaced_anchor" name="eqtn2"></a>\begin{eqnarray} \mbox{salida} = \left\{ \begin{array}{ll} 0 & \mbox{si } w\cdot x + b \leq 0 \\ 1 & \mbox{si } w\cdot x + b > 0 \end{array} \right. \tag{2}\end{eqnarray} Puedes pensar que bías es como una medida de lo fácil que es conseguir que el percetrón tenga a la salida un @1@, cuando la suma de las otras entradas es cero.
